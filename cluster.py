import os
import cv2
import shutil
import numpy as np
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from sklearn.metrics.pairwise import cosine_distances
from insightface.app import FaceAnalysis
import hdbscan
from collections import defaultdict

IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}

def is_image(p: Path) -> bool:
    return p.suffix.lower() in IMG_EXTS

def _win_long(path: Path) -> str:
    p = str(path.resolve())
    if os.name == "nt":
        return "\\\\?\\" + p if not p.startswith("\\\\?\\") else p
    return p

def imread_safe(path: Path):
    try:
        data = np.fromfile(_win_long(path), dtype=np.uint8)
        if data.size == 0:
            return None
        return cv2.imdecode(data, cv2.IMREAD_COLOR)
    except Exception:
        return None

def merge_clusters_by_centroid(
    embeddings: List[np.ndarray],
    owners: List[Path],
    raw_labels: np.ndarray,
    threshold: Optional[float] = None,
    auto_threshold: bool = True,  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–∫–ª—é—á–µ–Ω
    margin: float = 0.08,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –º–∞—Ä–∂–∞ –¥–ª—è –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    min_threshold: float = 0.15,  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
    max_threshold: float = 0.5,   # –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
    progress_callback=None
) -> Tuple[Dict[int, Set[Path]], Dict[Path, Set[int]]]:

    if progress_callback:
        progress_callback("üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–ª–∏–∑–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 92)

    cluster_embeddings: Dict[int, List[np.ndarray]] = defaultdict(list)
    cluster_paths: Dict[int, List[Path]] = defaultdict(list)

    for label, emb, path in zip(raw_labels, embeddings, owners):
        if label == -1:
            continue
        cluster_embeddings[label].append(emb)
        cluster_paths[label].append(path)

    centroids = {label: np.mean(embs, axis=0) for label, embs in cluster_embeddings.items()}
    labels = list(centroids.keys())

    if auto_threshold and threshold is None:
        pairwise = [cosine_distances([centroids[a]], [centroids[b]])[0][0]
                    for i, a in enumerate(labels) for b in labels[i+1:]]
        if pairwise:
            mean_dist = np.mean(pairwise)
            std_dist = np.std(pairwise)
            # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ –º–∏–Ω—É—Å –ø–æ–ª–æ–≤–∏–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            threshold = max(min_threshold, min(mean_dist - std_dist * 0.5, max_threshold))
            
            if progress_callback:
                progress_callback(f"üìè –ê–≤—Ç–æ-–ø–æ—Ä–æ–≥: {threshold:.3f} (—Å—Ä–µ–¥–Ω–µ–µ: {mean_dist:.3f}, —Å—Ç–¥: {std_dist:.3f})", 93)
        else:
            threshold = min_threshold
            if progress_callback:
                progress_callback(f"üìè –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {threshold:.3f}", 93)
    elif threshold is None:
        threshold = 0.25  # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    next_cluster_id = 0
    label_to_group = {}
    total = len(labels)

    for i, label_i in enumerate(labels):
        if progress_callback:
            percent = 93 + int((i + 1) / max(total, 1) * 2)
            progress_callback(f"üîÅ –°–ª–∏—è–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {percent}% ({i+1}/{total})", percent)

        if label_i in label_to_group:
            continue
        group = [label_i]
        for j in range(i + 1, len(labels)):
            label_j = labels[j]
            if label_j in label_to_group:
                continue
            dist = cosine_distances([centroids[label_i]], [centroids[label_j]])[0][0]
            if dist < threshold:
                group.append(label_j)

        for l in group:
            label_to_group[l] = next_cluster_id
        next_cluster_id += 1

    merged_clusters: Dict[int, Set[Path]] = defaultdict(set)
    cluster_by_img: Dict[Path, Set[int]] = defaultdict(set)

    for label, path in zip(raw_labels, owners):
        if label == -1:
            continue
        new_label = label_to_group[label]
        merged_clusters[new_label].add(path)
        cluster_by_img[path].add(new_label)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ: –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ø–∞—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –µ—â–µ —Ä–∞–∑
    if progress_callback:
        progress_callback("üîÑ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 95)
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    final_clusters: Dict[int, Set[Path]] = defaultdict(set)
    final_cluster_by_img: Dict[Path, Set[int]] = defaultdict(set)
    
    # –ö–æ–ø–∏—Ä—É–µ–º —Ç–µ–∫—É—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    for cluster_id, paths in merged_clusters.items():
        final_clusters[cluster_id] = paths.copy()
        for path in paths:
            final_cluster_by_img[path].add(cluster_id)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ø–∞—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞ —Å—Ö–æ–∂–µ—Å—Ç—å
    cluster_ids = list(final_clusters.keys())
    merged_any = True
    
    while merged_any:
        merged_any = False
        for i, cluster_i in enumerate(cluster_ids):
            if cluster_i not in final_clusters:
                continue
                
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞ i
            paths_i = list(final_clusters[cluster_i])
            embeddings_i = [emb for emb, path in zip(embeddings, owners) if path in paths_i]
            if not embeddings_i:
                continue
            centroid_i = np.mean(embeddings_i, axis=0)
            
            for j, cluster_j in enumerate(cluster_ids[i+1:], i+1):
                if cluster_j not in final_clusters:
                    continue
                    
                # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞ j
                paths_j = list(final_clusters[cluster_j])
                embeddings_j = [emb for emb, path in zip(embeddings, owners) if path in paths_j]
                if not embeddings_j:
                    continue
                centroid_j = np.mean(embeddings_j, axis=0)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞–º–∏
                dist = cosine_distances([centroid_i], [centroid_j])[0][0]
                
                # –ï—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–Ω—å—à–µ –ø–æ—Ä–æ–≥–∞, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
                if dist < threshold:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä j –≤ –∫–ª–∞—Å—Ç–µ—Ä i
                    final_clusters[cluster_i].update(final_clusters[cluster_j])
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º mapping –¥–ª—è –≤—Å–µ—Ö –ø—É—Ç–µ–π –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞ j
                    for path in paths_j:
                        final_cluster_by_img[path].discard(cluster_j)
                        final_cluster_by_img[path].add(cluster_i)
                    
                    # –£–¥–∞–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä j
                    del final_clusters[cluster_j]
                    cluster_ids.remove(cluster_j)
                    merged_any = True
                    break
            
            if merged_any:
                break
    
    if progress_callback:
        progress_callback(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(final_clusters)}", 98)
    
    return final_clusters, final_cluster_by_img

def build_plan_live(
    input_dir: Path,
    det_size=(640, 640),
    min_score: float = 0.4,  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥ –¥–µ—Ç–µ–∫—Ü–∏–∏
    min_cluster_size: int = 3,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
    min_samples: int = 2,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π
    providers: List[str] = ("CPUExecutionProvider",),
    progress_callback=None,
):
    input_dir = Path(input_dir)
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∏—Å–∫–ª—é—á–∞—è —Ç–µ, —á—Ç–æ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–∞–ø–∫–∞—Ö —Å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏
    excluded_names = ["–æ–±—â–∏–µ", "–æ–±—â–∞—è", "common", "shared", "–≤—Å–µ", "all", "mixed", "—Å–º–µ—à–∞–Ω–Ω—ã–µ"]
    all_images = [
        p for p in input_dir.rglob("*")
        if is_image(p)
        and not any(ex in str(p).lower() for ex in excluded_names)
    ]

    if progress_callback:
        progress_callback(f"üìÇ –°–∫–∞–Ω–∏—Ä—É–µ—Ç—Å—è: {input_dir}, –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)}", 1)

    app = FaceAnalysis(name="buffalo_l", providers=list(providers))
    ctx_id = -1 if "cpu" in str(providers).lower() else 0
    app.prepare(ctx_id=ctx_id, det_size=det_size)

    if progress_callback:
        progress_callback("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...", 10)

    embeddings = []
    owners = []
    img_face_count = {}
    unreadable = []
    no_faces = []

    total = len(all_images)
    processed_faces = 0
    
    for i, p in enumerate(all_images):
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if progress_callback:
            percent = 10 + int((i + 1) / max(total, 1) * 70)  # 10-80% –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            progress_callback(f"üì∑ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {percent}% ({i+1}/{total}) - {p.name}", percent)
        
        img = imread_safe(p)
        if img is None:
            unreadable.append(p)
            continue
            
        faces = app.get(img)
        if not faces:
            no_faces.append(p)
            continue

        count = 0
        for f in faces:
            if getattr(f, "det_score", 1.0) < min_score:
                continue
            emb = getattr(f, "normed_embedding", None)
            if emb is None:
                continue
            emb = emb.astype(np.float64)  # HDBSCAN expects double
            norm = np.linalg.norm(emb)
            if norm > 0:
                emb = emb / norm
            embeddings.append(emb)
            owners.append(p)
            count += 1
            processed_faces += 1

        if count > 0:
            img_face_count[p] = count

    if not embeddings:
        if progress_callback:
            progress_callback("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ª–∏—Ü –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏", 100)
        print(f"‚ö†Ô∏è –ù–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {input_dir}")
        return {
            "clusters": {},
            "plan": [],
            "unreadable": [str(p) for p in unreadable],
            "no_faces": [str(p) for p in no_faces],
        }

    # –≠—Ç–∞–ø 2: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    if progress_callback:
        progress_callback(f"üîÑ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è {len(embeddings)} –ª–∏—Ü...", 80)
    
    X = np.vstack(embeddings)
    distance_matrix = cosine_distances(X)

    if progress_callback:
        progress_callback("üîÑ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π...", 85)

    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∏—Ü
    num_faces = len(embeddings)
    adaptive_min_cluster_size = max(min_cluster_size, min(5, num_faces // 10))
    adaptive_min_samples = max(min_samples, min(3, num_faces // 20))
    
    if progress_callback:
        progress_callback(f"üìä –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: min_cluster_size={adaptive_min_cluster_size}, min_samples={adaptive_min_samples}", 87)

    model = hdbscan.HDBSCAN(
        metric='precomputed', 
        min_cluster_size=adaptive_min_cluster_size, 
        min_samples=adaptive_min_samples,
        cluster_selection_epsilon=0.1  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —Ä–∞–∑–º–µ—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    )
    raw_labels = model.fit_predict(distance_matrix)

    cluster_map, cluster_by_img = merge_clusters_by_centroid(
        embeddings=embeddings,
        owners=owners,
        raw_labels=raw_labels,
        auto_threshold=True,
        margin=0.08,  # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        min_threshold=0.15,
        max_threshold=0.5,
        progress_callback=progress_callback
    )

    # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    if progress_callback:
        progress_callback("üìä –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...", 94)
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    cluster_sizes = [len(paths) for paths in cluster_map.values()]
    avg_cluster_size = np.mean(cluster_sizes) if cluster_sizes else 0
    max_cluster_size = max(cluster_sizes) if cluster_sizes else 0
    min_cluster_size_actual = min(cluster_sizes) if cluster_sizes else 0
    
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
    print(f"   - –í—Å–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(cluster_map)}")
    print(f"   - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞: {avg_cluster_size:.1f}")
    print(f"   - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max_cluster_size}")
    print(f"   - –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min_cluster_size_actual}")
    
    if progress_callback:
        progress_callback(f"üìä –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(cluster_map)}, —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_cluster_size:.1f}", 95)

    # –≠—Ç–∞–ø 3: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    if progress_callback:
        progress_callback("üîÑ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è...", 96)
    
    plan = []
    for path in all_images:
        clusters = cluster_by_img.get(path)
        if not clusters:
            continue
        plan.append({
            "path": str(path),
            "cluster": sorted(list(clusters)),
            "faces": img_face_count.get(path, 0)
        })

    # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
    if progress_callback:
        progress_callback(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ù–∞–π–¥–µ–Ω–æ {len(cluster_map)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(plan)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", 100)

    print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {input_dir} ‚Üí –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(cluster_map)}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(plan)}")

    return {
        "clusters": {
            int(k): [str(p) for p in sorted(v, key=lambda x: str(x))]
            for k, v in cluster_map.items()
        },
        "plan": plan,
        "unreadable": [str(p) for p in unreadable],
        "no_faces": [str(p) for p in no_faces],
    }

def distribute_to_folders(plan: dict, base_dir: Path, cluster_start: int = 1, progress_callback=None) -> Tuple[int, int, int]:
    moved, copied = 0, 0
    moved_paths = set()

    used_clusters = sorted({c for item in plan.get("plan", []) for c in item["cluster"]})
    cluster_id_map = {old: cluster_start + idx for idx, old in enumerate(used_clusters)}

    plan_items = plan.get("plan", [])
    total_items = len(plan_items)
    
    if progress_callback:
        progress_callback(f"üîÑ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {total_items} —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–∞–ø–∫–∞–º...", 0)

    for i, item in enumerate(plan_items):
        if progress_callback:
            percent = int((i + 1) / max(total_items, 1) * 100)
            progress_callback(f"üìÅ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤: {percent}% ({i+1}/{total_items})", percent)
            
        src = Path(item["path"])
        clusters = [cluster_id_map[c] for c in item["cluster"]]
        if not src.exists():
            continue

        if len(clusters) == 1:
            cluster_id = clusters[0]
            dst = base_dir / f"{cluster_id}" / src.name
            dst.parent.mkdir(parents=True, exist_ok=True)
            try:
                shutil.move(str(src), str(dst))
                moved += 1
                moved_paths.add(src.parent)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è {src} ‚Üí {dst}: {e}")
        else:
            for cluster_id in clusters:
                dst = base_dir / f"{cluster_id}" / src.name
                dst.parent.mkdir(parents=True, exist_ok=True)
                try:
                    shutil.copy2(str(src), str(dst))
                    copied += 1
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {src} ‚Üí {dst}: {e}")
            try:
                src.unlink()  # —É–¥–∞–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –ø–æ—Å–ª–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞–ø–æ–∫
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {src}: {e}")

    # –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫
    if progress_callback:
        progress_callback("üßπ –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫...", 100)

    for p in sorted(moved_paths, key=lambda x: len(str(x)), reverse=True):
        try:
            if p.exists() and not any(p.iterdir()):
                p.rmdir()
        except Exception:
            pass

    print(f"üì¶ –ü–µ—Ä–µ–º–µ—â–µ–Ω–æ: {moved}, —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {copied}")
    return moved, copied, cluster_start + len(used_clusters)

def process_group_folder(group_dir: Path, progress_callback=None):
    cluster_counter = 1
    subfolders = [f for f in sorted(group_dir.iterdir()) if f.is_dir() and "–æ–±—â–∏–µ" not in f.name.lower()]
    total_subfolders = len(subfolders)
    
    for i, subfolder in enumerate(subfolders):
        if progress_callback:
            percent = 10 + int((i + 1) / max(total_subfolders, 1) * 80)
            progress_callback(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ–¥–ø–∞–ø–∫–∞: {subfolder.name} ({i+1}/{total_subfolders})", percent)
            
        print(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ–¥–ø–∞–ø–∫–∞: {subfolder}")
        plan = build_plan_live(subfolder)
        print(f"üìä –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(plan.get('clusters', {}))}, —Ñ–∞–π–ª–æ–≤: {len(plan.get('plan', []))}")
        moved, copied, cluster_counter = distribute_to_folders(plan, subfolder, cluster_start=cluster_counter)



